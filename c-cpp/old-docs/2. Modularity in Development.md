When systems can feel overwhelming when looking as a while, most development usually occurs in small parts at the system at the time.
This can be seen across the entire area.

The way to imagine most things is to think of them as modules.
A module has:
- Inputs - What do you send in.
- Outputs - What do you get out.
- States - What changed when you sent something
- Signals - When something is sent through the inputs.
By looking at a single part of a system and only narrowing down the options to the points above, its much easier to follow and find problems.
As a programmer, your job is to patch the flow of logic, so as little spills out.

---

## Circuits

In circuitry, cables are your inputs and outputs, and the flow of electricity is your state.
A circuit wont change unless you alter the state somehow with some signal change.
A lack of flow is also a state the circuits can take advantage of, 
However they wont trigger changes unless something actually is altered.

How a circuit or chip is designed has no significance, only how it acts with the signals you provide.

![[res/Pasted image 20251021185652.png]] 
How a Adder curcuit looks inside an IC, not really needed to know, but in the next slide...

### Things that can go wrong?
- Loose connections
- Not isolated enough


---

# Signaling

You can see that its simple a box, with its inputs and outputs. Like a function in a program.
![[res/Pasted image 20251021190049.png]]

Here we see that the signal is split into 4 different channels, representing the 4 bits of an adder per module, but with two modules, we suddenly have a 8 bit adder.
Here we actually group all 8 bits as a single "input" when zooming out in our module hierarchy.
#### Things that can go wrong?

- Integer Overflow
- Signal can be too fast, (all switches havent flipped fast enough.)
- Not enough power, etc.

Compared to the previous slide, we don't really care about the other problems. since we cant really do anything about it, its inside its own module, we just have to find another module that works instead. If it doesnt exist, then we can concider building our own or just find another solutions.

---

## CPUs
## CISC (X86, ARM)
![[res/Pasted image 20251021190844.png]]
For CISC / Personal computing, Performance and Flexibility is the main focus.
In this case, Data Latency is one key factor to performance, which means less travel distance normally.
In X86 and ARM systems, there exists a small memory bank which stores memory obtained from the RAM. 
Each Core usually loads about 64/128 bytes (not bits!) at a time from the cache, known as a Cache Line, then allows the CPU to do stuff to it until it sends it back.
- L1-cache, 0.5 ns, 20%
- L2-cache, 1.8 ns, 5%
- L3-cache, 4.2 ns, 1.5%
- Main memory, 70 ns, 0%
Looking at these times, you'd see that memory is about 50-100x faster in the cache, than in the ram. Meaning storing things in sequence in memory is a lot more performant.
---
![[res/Pasted image 20251021191848.png]]

This is a core module inside the system. These chips usually have multiple sections that can do the same operation and the software is smart enough to split multiple instructions from your code to various modules in parallel. 
In fact, it sometimes has enough room to do more stuff than what your code has assigned, so they took advantage of this by allowing multiple programs to run on the same core at the same time. 
This is usually known as hyperthreading, which makes your core count in the PC seem higher than what you physically have.
A CPU architecture can meassure its parallel efficiency with "Instructions per Cycle" (IPC)
### So wheres the catches?
- Since multiple cores work at the same time, they usually share the same cache.
  Meaning that when they send the data back, and another core uses the same data, they can overwrite each other in random order.
  Throwing multiple cores in the same segment of memory can be corrupted (unless its read-only). The way to solve this is that compilers can "align" the memory with some unused padding so they fit within a single cache line.
- Algorithms can sometimes be alot faster than just a simple sequence in memory, but combining both works best.
- Hyperthreading can sometimes be battling with the other processes about the cache and module allocation, causing it to slow down some code that are optimized to uses all modules, unpredictability which is also a trend in slow performance.

---
# RISC

RISC purpose usually isn't performance, but to be stable, cheap and use as little memory as possible.
Some Risc systems dont have caches,
Therefore the way to code for it is to get only the data you need and output it as fast as you can, high clock speeds benefits here alot.
Avoid using recursive functions (functions that calls it self), avoid allocating in repetitive loops, try to use and reuse fixed size buffers as much as possible.

